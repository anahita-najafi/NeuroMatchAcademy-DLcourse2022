{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anahita-najafi/NeuroMatchAcademy-DLcourse2022/blob/main/CNNs/Copy_of_W2D4_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "qmV8yWwT-_hp"
      },
      "source": [
        "# Tutorial 3: Conditional GANs and Implications of GAN Technology\n",
        "\n",
        "**Week 2, Day 4: Generative Models**\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Seungwook Han, Kai Xu, Akash Srivastava\n",
        "\n",
        "__Content reviewers:__ Polina Turishcheva, Melvin Selim Atay, Hadi Vafaei, Deepak Raya, Kelson Shilling-Scrivo\n",
        "\n",
        "__Content editors:__ Spiros Chavlis\n",
        "\n",
        "__Production editors:__ Arush Tagade, Gagana B, Spiros Chavlis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "CTR2tZ1o-_h5"
      },
      "source": [
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "s-RvtM71-_h8"
      },
      "source": [
        "---\n",
        "\n",
        "## Tutorial Objectives\n",
        "\n",
        "The goal of this tutorial is to understand conditional GANs. Then you will have the opportunity to experience first-hand how effective GANs are at modeling the data distribution and to question what the consequences of this technology may be.\n",
        "\n",
        "By the end of this tutorial you will be able to:\n",
        "- Understand the differences in conditional GANs.\n",
        "- Generate high-dimensional natural images from a BigGAN.\n",
        "- Understand the efficacy of GANs in modeling the data distribution (e.g., faces).\n",
        "- Understand the energy inefficiency / environmental impact of training these large generative models.\n",
        "- Understand the implications of this technology (ethics, environment, *etc*.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "mG8G5Uwn-_iA"
      },
      "outputs": [],
      "source": [
        "# @title Tutorial slides\n",
        "\n",
        "from IPython.display import IFrame\n",
        "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/ps28k/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kWogCTHS-_iE"
      },
      "source": [
        "These are the slides for the videos in this tutorial. If you want to locally download the slides, click [here](https://osf.io/ps28k/download)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0IM7-Odk-_iG"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "z9-lPujB-_iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357bf901-6cc3-4e6e-d359-1d5fbe3469bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 132 kB 11.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 31.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 139 kB 50.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 55.3 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h  Building wheel for libsixel-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for evaltools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "# @markdown Install *Huggingface BigGAN* library\n",
        "!pip install pytorch-pretrained-biggan --quiet\n",
        "!pip install Pillow libsixel-python --quiet\n",
        "!pip install nltk --quiet\n",
        "\n",
        "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
        "\n",
        "from evaltools.airtable import AirtableForm\n",
        "atform = AirtableForm('appn7VdPRseSoMXEG', 'W2D4_T3','https://portal.neuromatchacademy.org/api/redirect/to/ddf809a2-5590-4d71-a764-1572e85dce27')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "8IOuwkXB-_iJ"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pytorch_pretrained_biggan import BigGAN\n",
        "from pytorch_pretrained_biggan import one_hot_from_names\n",
        "from pytorch_pretrained_biggan import truncated_noise_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "NLT4cfms-_iL"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "import ipywidgets as widgets       # Interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "uEOotXYA-_iN"
      },
      "outputs": [],
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# for DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "el5Wi-wc-_iR"
      },
      "outputs": [],
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\n",
        "# especially if torch modules used.\n",
        "\n",
        "# Inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "GE85Aypw-_iT"
      },
      "outputs": [],
      "source": [
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "_mlkGV7Q-_iU"
      },
      "outputs": [],
      "source": [
        "# @title Download `wordnet` dataset\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "import os, requests, zipfile\n",
        "\n",
        "os.environ['NLTK_DATA'] = 'nltk_data/'\n",
        "\n",
        "fnames = ['wordnet.zip', 'omw-1.4.zip']\n",
        "urls = ['https://osf.io/ekjxy/download', 'https://osf.io/kuwep/download']\n",
        "\n",
        "for fname, url in zip(fnames, urls):\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "  with open(fname, 'wb') as fd:\n",
        "    fd.write(r.content)\n",
        "\n",
        "  with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
        "    zip_ref.extractall('nltk_data/corpora')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "tm22x-r_-_iV"
      },
      "source": [
        "---\n",
        "# Section 1: Generating with a conditional GAN (BigGAN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "HAXhlyo2-_iW"
      },
      "outputs": [],
      "source": [
        "# @title Video 1: Conditional Generative Models\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1f54y1E79D\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"lV6zH2xDZck\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 1: Conditional Generative Models')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "sVVtn_f6-_iX"
      },
      "source": [
        "In this section, we will load a pre-trained conditional GAN, BigGAN, which is the state-of-the-art model in conditional high-dimensional natural image generation, and generate samples from it. Since it is a class conditional model, we will be able to use the class label to generate images from the different classes of objects.\n",
        "\n",
        "Read here for more details on BigGAN: [Brock et al., 2019](https://arxiv.org/abs/1809.11096)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "6iUVF2gG-_iY"
      },
      "outputs": [],
      "source": [
        "def load_biggan(model_res):\n",
        "  \"\"\"\n",
        "  Load respective BigGAN model for the specified resolution (biggan-deep-128, biggan-deep-256, biggan-deep-512)\n",
        "  \"\"\"\n",
        "  return BigGAN.from_pretrained('biggan-deep-{}'.format(model_res))\n",
        "\n",
        "\n",
        "def create_class_noise_vectors(class_str, trunc, num_samples):\n",
        "  \"\"\"\n",
        "  Create class and noise vectors for sampling from BigGAN\n",
        "\n",
        "  Args:\n",
        "    class_str: string\n",
        "      Class\n",
        "    trunc: float\n",
        "      Truncation factor\n",
        "    num_samples: int\n",
        "      Number of samples\n",
        "\n",
        "  Returns:\n",
        "    class_vector: np.ndarray\n",
        "      Class vector sampled from BigGan\n",
        "    noise_vector: np.ndarray\n",
        "      Noise vector\n",
        "  \"\"\"\n",
        "  class_vector = one_hot_from_names([class_str]*num_samples, batch_size=num_samples)\n",
        "  noise_vector = truncated_noise_sample(truncation=trunc, batch_size=num_samples)\n",
        "\n",
        "  return class_vector, noise_vector\n",
        "\n",
        "def generate_biggan_samples(model, class_vector, noise_vector, device,\n",
        "                            truncation=0.4):\n",
        "  \"\"\"\n",
        "  Generate samples from BigGAN\n",
        "\n",
        "  Args:\n",
        "    model: nn.module\n",
        "      Model\n",
        "    device: string\n",
        "      GPU if available. CPU otherwise.\n",
        "    truncation: float\n",
        "      Truncation factor\n",
        "    class_vector: np.ndarray\n",
        "      Class vector sampled from BigGan\n",
        "    noise_vector: np.ndarray\n",
        "      Noise vector\n",
        "\n",
        "  Returns:\n",
        "    output_grid: torch.tensor\n",
        "      Make grid and display generated samples\n",
        "  \"\"\"\n",
        "  # Convert to tensor\n",
        "  noise_vector = torch.from_numpy(noise_vector)\n",
        "  class_vector = torch.from_numpy(class_vector)\n",
        "\n",
        "  # Move to GPU\n",
        "  noise_vector = noise_vector.to(device)\n",
        "  class_vector = class_vector.to(device)\n",
        "  model.to(device)\n",
        "\n",
        "  # Generate an image\n",
        "  with torch.no_grad():\n",
        "      output = model(noise_vector, class_vector, truncation)\n",
        "\n",
        "  # Back to CPU\n",
        "  output = output.to('cpu')\n",
        "\n",
        "  # The output layer of BigGAN has a tanh layer, resulting the range of [-1, 1] for the output image\n",
        "  # Therefore, we normalize the images properly to [0, 1] range.\n",
        "  # Clipping is only in case of numerical instability problems\n",
        "\n",
        "  output = torch.clip(((output.detach().clone() + 1) / 2.0), 0, 1)\n",
        "  output = output\n",
        "\n",
        "  # Make grid and show generated samples\n",
        "  output_grid = torchvision.utils.make_grid(output,\n",
        "                                            nrow=min(4, output.shape[0]),\n",
        "                                            padding=5)\n",
        "  plt.imshow(output_grid.permute(1, 2, 0))\n",
        "\n",
        "  return output_grid\n",
        "\n",
        "\n",
        "def generate(b):\n",
        "  \"\"\"\n",
        "  Generation function\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  # Create BigGAN model\n",
        "  model = load_biggan(MODEL_RESOLUTION)\n",
        "\n",
        "  # Use specified parameters (resolution, class, number of samples, etc) to generate from BigGAN\n",
        "  class_vector, noise_vector = create_class_noise_vectors(CLASS, TRUNCATION,\n",
        "                                                          NUM_SAMPLES)\n",
        "  samples_grid = generate_biggan_samples(model, class_vector, noise_vector,\n",
        "                                         DEVICE, TRUNCATION)\n",
        "  torchvision.utils.save_image(samples_grid, 'samples.png')\n",
        "  ### If CUDA out of memory issue, lower NUM_SAMPLES (number of samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "96zWmJV9-_iZ"
      },
      "source": [
        "## Section 1.1:  Define configurations\n",
        "\n",
        "We will now define the configurations (resolution of model, number of samples, class to sample from, truncation level) under which we will sample from BigGAN. \n",
        "\n",
        "***Question***: What is the truncation trick employed by BigGAN? How does sample variety and fidelity change by varying the truncation level? (Hint: play with the truncation slider and try sampling at different levels) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "FLvSrEX1-_ia"
      },
      "outputs": [],
      "source": [
        "# @title { run: \"auto\" }\n",
        "\n",
        "### RUN THIS BLOCK EVERY TIME YOU CHANGE THE PARAMETERS FOR GENERATION\n",
        "\n",
        "# Resolution at which to generate\n",
        "MODEL_RESOLUTION = \"128\" # @param [128, 256, 512]\n",
        "\n",
        "# Number of images to generate\n",
        "NUM_SAMPLES = 4 # @param {type:\"slider\", min:4, max:12, step:4}\n",
        "\n",
        "# Class of images to generate\n",
        "CLASS = 'German shepherd'  # @param ['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee', 'acoustic guitar', 'coffee mug', 'minibus', 'monitor']\n",
        "\n",
        "# Truncation level of the normal distribution we sample z from\n",
        "TRUNCATION = 0.4  # @param {type:\"slider\", min:0.1, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "suW-R-xp-_ic"
      },
      "outputs": [],
      "source": [
        "# @title Generate\n",
        "# Create generate button, given parameters specified above\n",
        "button = widgets.Button(description=\"GENERATE!\",\n",
        "                        layout=widgets.Layout(width='30%', height='80px'),\n",
        "                        button_style='danger')\n",
        "output = widgets.Output()\n",
        "display(button, output)\n",
        "button.on_click(generate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-GagSjW2-_ic"
      },
      "source": [
        "## Think! 1: BigGANs\n",
        "\n",
        "1. How does BigGAN differ from previous state-of-the-art generative models for high-dimensional natural images? In other words, how does BigGAN solve high-dimensional image generation? (Hint: look into model architecture and training configurations) (BigGAN paper: [Brock et al., 2018](https://arxiv.org/abs/1809.11096))\n",
        "2. Continuing from Question 1, what are the drawbacks of introducing such techniques into training large models for high-dimensional, diverse datasets?\n",
        "3. Play with other pre-trained generative models like StyleGAN here -- where code for sampling and interpolation in the latent space is available [here](https://github.com/NVlabs/stylegan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "-kybNoez-_id"
      },
      "outputs": [],
      "source": [
        "# @title Student Response\n",
        "from ipywidgets import widgets\n",
        "\n",
        "\n",
        "text=widgets.Textarea(\n",
        "   value='Type answer here and Push submit',\n",
        "   placeholder='Type something',\n",
        "   description='',\n",
        "   disabled=False\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Submit!\")\n",
        "\n",
        "display(text,button)\n",
        "\n",
        "def on_button_clicked(b):\n",
        "   atform.add_answer('q1' , text.value)\n",
        "   print(\"Submission successful!\")\n",
        "\n",
        "\n",
        "button.on_click(on_button_clicked)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ksawlu2a-_ie"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D4_GenerativeModels/solutions/W2D4_Tutorial3_Solution_8bfa4c9b.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "bDtW2daS-_ig"
      },
      "source": [
        "---\n",
        "# Section 2: Ethical issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "DdvlYKmc-_ih"
      },
      "outputs": [],
      "source": [
        "# @title Video 2: Ethical Issues\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV11L411H7pr\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"ZtWFeUZgfVk\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 2: Ethical Issues')\n",
        "\n",
        "display(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Ru7--Jfg-_ii"
      },
      "source": [
        "## Section 2.1: Faces Quiz \n",
        "\n",
        "Now is your turn to test your abilities on recognizing a real vs. a fake image!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "U2D4qfQX-_ii"
      },
      "outputs": [],
      "source": [
        "# @markdown Real or Fake?\n",
        "from IPython.display import IFrame\n",
        "IFrame(src='https://docs.google.com/forms/d/e/1FAIpQLSeGjn2S2bn6Q1qWjVgDS5LG7G1GsQQh2Q0T9dEUO1z5_W0yYg/viewform?usp=sf_link', width=900, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ziSK0S5S-_ij"
      },
      "source": [
        "## Section 2.2: Energy Efficiency Quiz "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "obv0_by0-_ik"
      },
      "outputs": [],
      "source": [
        "# @markdown Make a guess\n",
        "IFrame(src='https://docs.google.com/forms/d/e/1FAIpQLSe8suNt4ZmadSr_6IWq6s_nUYxC1VCpjR2cBBmQ7cR_5znCZw/viewform?usp=sf_link', width=900, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "X3QGnAfB-_ik"
      },
      "source": [
        "---\n",
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Re4jWVyQ-_il"
      },
      "source": [
        "Hooray! You have finished the second week of NMA-DL!!!\n",
        "\n",
        "In the first section of this tutorial, we have learned: \n",
        "- How conditional GANs differ from unconditional models\n",
        "- How to use a pre-trained BigGAN model to generate high-dimensional photo-realistic images and its tricks to modulate diversity and image fidelity\n",
        "\n",
        "In the second section, we learned about the broader ethical implications of GAN technology on society through deepfakes and their tremendous energy inefficiency.\n",
        "\n",
        "On the brighter side, as we learned throughout the week, GANs are very effective in modeling the data distribution and have many practical applications.\n",
        "\n",
        "For example, as personalized healthcare and applications of AI in healthcare rise, the need to remove any Personally Identifiable Information (PII) becomes more important. As shown in [Piacentino and Angulo, 2020](https://doi.org/10.1007/978-3-030-45385-5_36), GANs can be leveraged to anonymize healthcare data.\n",
        "\n",
        "As a food for thought, what are some other practical applications of GANs that you can think of? Discuss with your pod your ideas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "lfIfu7-N-_im",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "30e45591-0afc-434e-e609-c6f187e5cc3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              " <div>\n",
              "   <a href= \"https://portal.neuromatchacademy.org/api/redirect/to/ddf809a2-5590-4d71-a764-1572e85dce27?data=eyJmb3JtX2lkIjogImFwcG43VmRQUnNlU29NWEVHIiwgInRhYmxlX25hbWUiOiAiVzJENF9UMyIsICJhbnN3ZXJzIjoge30sICJldmVudHMiOiBbeyJldmVudCI6ICJpbml0IiwgInRzIjogMTY1ODM5NDA4Mi43MTI5MjE5fSwgeyJldmVudCI6ICJ1cmwgZ2VuZXJhdGVkIiwgInRzIjogMTY1ODM5NDEwOS42NDQxMzd9XX0%3D\" target=\"_blank\">\n",
              "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\"\n",
              " alt=\"button link end of day Survey\" style=\"width:410px\"></a>\n",
              "   </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# @title Airtable Submission Link\n",
        "from IPython import display as IPydisplay\n",
        "IPydisplay.HTML(\n",
        "   f\"\"\"\n",
        " <div>\n",
        "   <a href= \"{atform.url()}\" target=\"_blank\">\n",
        "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\"\n",
        " alt=\"button link end of day Survey\" style=\"width:410px\"></a>\n",
        "   </div>\"\"\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "bnpi-Iwf-_in"
      },
      "outputs": [],
      "source": [
        "# @title (Bonus) Video 3: Recap and advanced topics\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1Uo4y1D7Nj\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"7nUjFG3N04I\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# Add event to airtable\n",
        "atform.add_event('Video 3: Recap and advanced topics')\n",
        "\n",
        "display(out)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}